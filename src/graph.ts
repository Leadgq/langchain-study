import { ChatOpenAI } from "@langchain/openai";
import { HumanMessage, AIMessage } from "@langchain/core/messages";
import { StateGraph, MessagesAnnotation } from "@langchain/langgraph";

// åˆ›å»º LLM å®ä¾‹
const llm = new ChatOpenAI({
  model: "gpt-3.5-turbo",
  temperature: 0.7,
});

// ä»£ç†èŠ‚ç‚¹
async function agentNode(state: any) {
  console.log("ğŸ¤– ä»£ç†èŠ‚ç‚¹æ­£åœ¨å¤„ç†...");
  const response = await llm.invoke(state.messages);
  return {
    messages: [response],
  };
}

// å†³ç­–èŠ‚ç‚¹
async function shouldContinueNode(state: any) {
  console.log("ğŸ” å†³ç­–èŠ‚ç‚¹æ­£åœ¨åˆ†æ...");
  const lastMessage = state.messages[state.messages.length - 1];

  // ç®€å•çš„å†³ç­–é€»è¾‘
  const messageContent = lastMessage.content.toString().toLowerCase();
  if (messageContent.includes("ç»§ç»­") || messageContent.includes("continue")) {
    return "agent";
  } else {
    return "end";
  }
}

// åˆ›å»ºå›¾
export const agent = () => {
  // ä½¿ç”¨ MessagesAnnotation æ¥åˆ›å»ºæœ‰çŠ¶æ€çš„å›¾
  const workflow = new StateGraph(MessagesAnnotation)
    .addNode("agent", agentNode)
    .addNode("should_continue", shouldContinueNode)
    .addEdge("__start__", "agent")
    .addEdge("agent", "should_continue")
    .addConditionalEdges(
      "should_continue",
      shouldContinueNode,
      {
        agent: "agent",
        end: "__end__"
      }
    );

  const app = workflow.compile();
  return app;
};

// é»˜è®¤å¯¼å‡º
export default agent;